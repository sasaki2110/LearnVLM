import gymnasium as gym
from gymnasium import spaces
import pybullet as p
import pybullet_data
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback

# --- 1. TensorBoardãƒ­ã‚°å‡ºåŠ›ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
class TensorboardCallback(BaseCallback):
    def _on_step(self) -> bool:
        if len(self.locals["infos"]) > 0:
            info = self.locals["infos"][0]
            for key, value in info.items():
                if key.startswith("custom/"):
                    self.logger.record(key, value)
        return True

# --- 2. å­¦ç¿’ç’°å¢ƒã®å®šç¾© ---
class Vision60TrotEnv(gym.Env):
    def __init__(self, render_mode=None):
        super().__init__()
        self.client = p.connect(p.GUI if render_mode == "human" else p.DIRECT)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        
        self.action_space = spaces.Box(low=-1, high=1, shape=(12,), dtype=np.float32)
        # è¦³æ¸¬: é«˜ã•(1), Zé€Ÿåº¦(1), å§¿å‹¢(3), ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆ(12), ãƒ•ã‚§ãƒ¼ã‚º(1) = 18æ¬¡å…ƒ
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(18,), dtype=np.float32)
        
        self.joint_indices = [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14]
        self.step_count = 0

    def reset(self, seed=None, options=None):
        p.resetSimulation(physicsClientId=self.client)
        p.setGravity(0, 0, -9.81)
        p.loadURDF("plane.urdf")
        
        # 1. ã‚¹ãƒãƒ¼ãƒ³ä½ç½®ã‚’ä½ãè¨­å®šï¼ˆ0.3mï¼‰
        spawn_height = 0.3
        self.robot_id = p.loadURDF("quadruped/vision60.urdf", [0, 0, spawn_height])
        
        self.step_count = 0
        
        # 2. åˆæœŸå§¿å‹¢ï¼šè†ã‚’1.0æ›²ã’ã€Abduction(è‚©)ã‚’ã€Œãƒã®å­—(0.2)ã€ã«é–‹ã
        knee_angle = 1.0
        abd_angle = 0.2
        
        # Abductionã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®ç¬¦å·ã‚’å€‹åˆ¥ã«è¨­å®šï¼ˆcheck_vision.pyã¨åŒã˜è¨­å®šï¼‰
        # i=0, j_idx=0: å·¦å‰ï¼ˆFLï¼‰
        # i=3, j_idx=4: å³å‰ï¼ˆFRï¼‰
        # i=6, j_idx=8: å·¦å¾Œã‚ï¼ˆRLï¼‰
        # i=9, j_idx=12: å³å¾Œã‚ï¼ˆRRï¼‰
        abd_signs = {
            0: 1.0,   # å·¦å‰ï¼ˆFLï¼‰: +1.0ã§ãƒ—ãƒ©ã‚¹ã€-1.0ã§ãƒã‚¤ãƒŠã‚¹
            3: 1.0,   # å³å‰ï¼ˆFRï¼‰: +1.0ã§ãƒ—ãƒ©ã‚¹ã€-1.0ã§ãƒã‚¤ãƒŠã‚¹
            6: -1.0,  # å·¦å¾Œã‚ï¼ˆRLï¼‰: +1.0ã§ãƒ—ãƒ©ã‚¹ã€-1.0ã§ãƒã‚¤ãƒŠã‚¹
            9: -1.0,  # å³å¾Œã‚ï¼ˆRRï¼‰: +1.0ã§ãƒ—ãƒ©ã‚¹ã€-1.0ã§ãƒã‚¤ãƒŠã‚¹
        }
        
        for i, j_idx in enumerate(self.joint_indices):
            if i in [0, 3, 6, 9]: # Abductionã‚¸ãƒ§ã‚¤ãƒ³ãƒˆ (ãƒã®å­—)
                init_pos = abd_angle * abd_signs[i]
            elif i in [2, 5, 8, 11]: # Knee
                init_pos = knee_angle
            else: # Hip
                init_pos = 0.0
            
            p.resetJointState(self.robot_id, j_idx, init_pos)
            # åˆæœŸçŠ¶æ…‹ã§å´©ã‚Œãªã„ã‚ˆã†ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚’ä¿æŒ
            p.setJointMotorControl2(self.robot_id, j_idx, p.POSITION_CONTROL, 
                                    targetPosition=init_pos, force=150.0)

        # 3. å®‰å®šå¾…ã¡ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ100ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        # ã“ã‚Œã«ã‚ˆã‚Šã€Œç€åœ°ã®è¡æ’ƒã€ãŒåã¾ã£ã¦ã‹ã‚‰å­¦ç¿’ãŒã‚¹ã‚¿ãƒ¼ãƒˆã—ã¾ã™
        for _ in range(100):
            p.stepSimulation()
        
        return self._get_obs(), {}

    def _get_obs(self):
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        vel, _ = p.getBaseVelocity(self.robot_id)
        euler = p.getEulerFromQuaternion(orn)
        joint_angles = [p.getJointState(self.robot_id, i)[0] for i in self.joint_indices]
        # ãƒˆãƒ­ãƒƒãƒˆç”¨ã®åŸºæº–ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ1.5Hzï¼‰
        phase = np.sin(2 * np.pi * 1.5 * (self.step_count * 0.01)) 
        return np.array([pos[2], vel[2]] + list(euler) + joint_angles + [phase], dtype=np.float32)

    def step(self, action):
        self.step_count += 1
        t = self.step_count * 0.01
        phase_a = np.sin(2 * np.pi * 1.5 * t)
        phase_b = -phase_a

        # é–¢ç¯€åˆ¶å¾¡ï¼ˆã—ãªã‚„ã‹è¨­å®šï¼‰
        for i, j_idx in enumerate(self.joint_indices):
            # åŸºæœ¬å§¿å‹¢ã®ç¶­æŒ
            if i in [0, 3, 6, 9]: target_pos = 0.2 if i in [0, 6] else -0.2 # ãƒã®å­—ä¿æŒ
            elif i in [2, 8]: target_pos = 1.0 + phase_a * 0.4 # FR, RL knee
            elif i in [5, 11]: target_pos = 1.0 + phase_b * 0.4 # FL, RR knee
            else: target_pos = 0.0
            
            # AIã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŠ ç®—
            target_pos += action[i] * 0.2
            
            # positionGainã‚’ä¸‹ã’ã¦ã€ŒæŸ”ã‚‰ã‹ãã€ã€velocityGainã‚’ä¸Šã’ã¦ã€Œç²˜ã‚Šã€ã‚’å‡ºã™
            p.setJointMotorControl2(
                self.robot_id, j_idx, p.POSITION_CONTROL, 
                targetPosition=target_pos, 
                force=100.0, 
                positionGain=0.05, 
                velocityGain=1.5
            )

        p.stepSimulation()
        obs = self._get_obs()
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        euler = p.getEulerFromQuaternion(orn)
        
        # åˆ¤å®š
        # 1.5ç§’é–“ï¼ˆ150ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã¯è»¢å€’åˆ¤å®šã‚’è¡Œã‚ãªã„ã€ŒåŸ·è¡ŒçŒ¶äºˆã€
        is_falling = (pos[2] < 0.28 or abs(euler[0]) > 0.6 or abs(euler[1]) > 0.6)
        
        terminated = False
        reward = 1.0 # ç”Ÿå­˜å ±é…¬ï¼ˆå›ºå®šå€¤ï¼‰
        
        if self.step_count > 150 and is_falling:
            terminated = True
            reward = -100.0
        
        # è£œåŠ©å ±é…¬
        knee_fl = p.getJointState(self.robot_id, self.joint_indices[5])[0]
        knee_fr = p.getJointState(self.robot_id, self.joint_indices[2])[0]
        knee_diff = abs(knee_fl - knee_fr)
        
        if not terminated:
            # é«˜ã•ç¶­æŒï¼ˆé‡ã¿ã‚’5.0ã«è»½æ¸›ï¼‰
            reward -= abs(pos[2] - 0.4) * 5.0
            # å·¦å³å¯¾ç§°ç¦æ­¢å ±é…¬
            reward += (knee_diff - 0.2) * 10.0 
            # å‰é€²å ±é…¬
            vel, _ = p.getBaseVelocity(self.robot_id)
            reward += vel[0] * 30.0

        info = {
            "custom/knee_diff": knee_diff,
            "custom/height": pos[2],
            "custom/roll": abs(euler[0])
        }

        return obs, reward, terminated, False, info

# --- 3. å­¦ç¿’å®Ÿè¡Œ ---
if __name__ == "__main__":
    # æ—¢å­˜ã®ãƒ­ã‚°ãŒã‚ã‚‹å ´åˆã¯å‰Šé™¤ã™ã‚‹ã‹ãƒ•ã‚©ãƒ«ãƒ€åã‚’å¤‰ãˆã‚‹ã“ã¨ã‚’æ¨å¥¨
    env = Vision60TrotEnv() 
    
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1, 
        device="cpu",
        tensorboard_log="./ppo_vision_logs/"
    )
    
    callback = TensorboardCallback()
    print("ğŸ¾ å®‰å®šåŒ–ãƒªã‚»ãƒƒãƒˆï¼†ã—ãªã‚„ã‹åˆ¶å¾¡ã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model.learn(total_timesteps=500000, callback=callback)
    model.save("ppo_vision60_stable_v1")