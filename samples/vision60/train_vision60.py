import gymnasium as gym
from gymnasium import spaces
import pybullet as p
import pybullet_data
import numpy as np
import os
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv

# --- 1. ç’°å¢ƒã®å®šç¾© ---
class Vision60PositionEnv(gym.Env):
    def __init__(self, render_mode=None):
        super().__init__()
        self.client = p.connect(p.GUI if render_mode == "human" else p.DIRECT)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        
        # AIã®å‡ºåŠ›: 12å€‹ã®é–¢ç¯€ã®ã€Œç›®æ¨™è§’åº¦ã€ (-1 ~ 1)
        self.action_space = spaces.Box(low=-1, high=1, shape=(12,), dtype=np.float32)
        # è¦³æ¸¬: é«˜ã•(1), Zé€Ÿåº¦(1), å§¿å‹¢(3), ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆè§’(12) = 17æ¬¡å…ƒ
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(17,), dtype=np.float32)
        
        self.joint_indices = [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14]

    def reset(self, seed=None, options=None):
        p.resetSimulation(physicsClientId=self.client)
        p.setGravity(0, 0, -9.81)
        p.loadURDF("plane.urdf")
        
        # 0.5mã®é«˜ã•ã‹ã‚‰ç”Ÿæˆ
        self.robot_id = p.loadURDF("quadruped/vision60.urdf", [0, 0, 0.5])
        
        # åˆæœŸå§¿å‹¢: å°‘ã—è†ã‚’æ›²ã’ã¦ãŠãã¨ç«‹ã¡ä¸ŠãŒã‚Šã‚„ã™ã„
        for i, j_idx in enumerate(self.joint_indices):
            # è†(Knee)ã¯Index 2, 5, 8, 11 (ãƒªã‚¹ãƒˆå†…ã§ã¯ 2, 5, 8, 11ç•ªç›®)
            if i in [2, 5, 8, 11]:
                p.resetJointState(self.robot_id, j_idx, 1.0) 
            
        return self._get_obs(), {}

    def _get_obs(self):
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        vel, _ = p.getBaseVelocity(self.robot_id)
        euler = p.getEulerFromQuaternion(orn)
        joint_angles = [p.getJointState(self.robot_id, i)[0] for i in self.joint_indices]
        return np.array([pos[2], vel[2]] + list(euler) + joint_angles, dtype=np.float32)

    def step(self, action):
        # AIã®å‡ºåŠ›ã‚’å®Ÿéš›ã®é–¢ç¯€è§’åº¦(ãƒ©ã‚¸ã‚¢ãƒ³)ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        for i, j_idx in enumerate(self.joint_indices):
            if i in [0, 3, 6, 9]: # Abduction (Â±0.43 rad)
                target_pos = action[i] * 0.43
                force = 300.0
            elif i in [1, 4, 7, 10]: # Hip (Â±3.14 rad)
                target_pos = action[i] * 3.14
                force = 80.0
            else: # Knee (0 ~ 3.14 rad)
                # action[-1, 1] -> [0, 3.14]
                target_pos = (action[i] + 1) * 1.57
                force = 80.0

            p.setJointMotorControl2(
                self.robot_id, j_idx, p.POSITION_CONTROL, 
                targetPosition=target_pos, force=force
            )
        
        p.stepSimulation()
        obs = self._get_obs()
        
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        euler = p.getEulerFromQuaternion(orn)
        vel, ang_vel = p.getBaseVelocity(self.robot_id)
        
        # 1. çµ‚äº†åˆ¤å®šã®å¼·åŒ–ï¼ˆã‚¹ãƒ‘ãƒ«ã‚¿ï¼‰
        # é«˜ã•ãŒ0.35mä»¥ä¸‹ã€ã¾ãŸã¯ Roll/PitchãŒ45åº¦ä»¥ä¸Šå‚¾ã„ãŸã‚‰å³ãƒªã‚»ãƒƒãƒˆ
        terminated = pos[2] < 0.35 or abs(euler[0]) > 0.8 or abs(euler[1]) > 0.8
        
        # 2. å ±é…¬è¨­è¨ˆï¼ˆãƒãƒŠãƒ¼é‡è¦–ï¼‰
        reward = 1.0  # ç”Ÿå­˜
        
        # å‰é€²å ±é…¬ï¼ˆå§¿å‹¢ãŒè‰¯ã„æ™‚ã ã‘é«˜é¡ã«ï¼‰
        if abs(euler[0]) < 0.3 and abs(euler[1]) < 0.3:
            reward += vel[0] * 200.0  # ã¾ã£ã™ãç«‹ã£ã¦é€²ã‚€ãªã‚‰è¶…ãƒœãƒ¼ãƒŠã‚¹
        else:
            reward += vel[0] * 20.0   # è»¢ã³ã‹ã‘ãªãŒã‚‰é€²ã‚€ã®ã¯å°‘é¡
            
        # å›è»¢ã¸ã®ç½°å‰‡ï¼ˆã¾ã£ã™ãé€²ã¾ã›ã‚‹ï¼‰
        reward -= abs(ang_vel[2]) * 10.0  
        # å·¦å³ã¸ã®ãƒ•ãƒ©ã¤ãç½°å‰‡
        reward -= abs(vel[1]) * 10.0
        
        return obs, reward, terminated, False, {}

    def close(self):
        p.disconnect(self.client)

# --- 2. å­¦ç¿’ã®å®Ÿè¡Œ ---
def make_env(rank):
    def _init(): return Vision60PositionEnv()
    return _init

if __name__ == "__main__":
    num_cpu = 4
    env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’å°‘ã—æ·±ãã—ã¦ã€è¤‡é›‘ãªå§¿å‹¢ã‚’å­¦ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹
    policy_kwargs = dict(net_arch=[256, 256])
    
    #model = PPO("MlpPolicy", env, policy_kwargs=policy_kwargs, verbose=1, device="cuda")
    model = PPO("MlpPolicy", env, policy_kwargs=policy_kwargs, verbose=1, device="cpu")
    
    print("ğŸ• Vision60 ä½ç½®åˆ¶å¾¡(Position Control)ã§ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model.learn(total_timesteps=500000)
    model.save("ppo_vision60_position_step")
    print("âœ… å­¦ç¿’å®Œäº†ã€‚ ppo_vision60_position_step.zip ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")