import gymnasium as gym
from gymnasium import spaces
import pybullet as p
import pybullet_data
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback

# --- 1. TensorBoardã«ã‚«ã‚¹ã‚¿ãƒ å ±é…¬ã‚’é€ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
class TensorboardCallback(BaseCallback):
    def _on_step(self) -> bool:
        # infoè¾æ›¸ã«å…¥ã‚ŒãŸ custom/ ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²
        if len(self.locals["infos"]) > 0:
            info = self.locals["infos"][0]
            for key, value in info.items():
                if key.startswith("custom/"):
                    self.logger.record(key, value)
        return True

# --- 2. å­¦ç¿’ç’°å¢ƒã®å®šç¾© ---
class Vision60TrotEnv(gym.Env):
    def __init__(self, render_mode=None):
        super().__init__()
        self.client = p.connect(p.GUI if render_mode == "human" else p.DIRECT)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        
        self.action_space = spaces.Box(low=-1, high=1, shape=(12,), dtype=np.float32)
        # è¦³æ¸¬: é«˜ã•(1), Zé€Ÿåº¦(1), å§¿å‹¢(3), ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆ(12), ãƒ•ã‚§ãƒ¼ã‚º(1) = 18æ¬¡å…ƒ
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(18,), dtype=np.float32)
        
        self.joint_indices = [0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14]
        self.step_count = 0

    def reset(self, seed=None, options=None):
        p.resetSimulation(physicsClientId=self.client)
        p.setGravity(0, 0, -9.81)
        p.loadURDF("plane.urdf")
        
        # åˆæœŸå§¿å‹¢ã‚’å°‘ã—é«˜ãã—ã€ã‚ãšã‹ã«ãƒ©ãƒ³ãƒ€ãƒ ãªå‚¾ãã‚’ä¸ãˆã¦ã€Œã©ã‚„é¡”ã€ã‚’é˜²æ­¢
        rand_orn = p.getQuaternionFromEuler([np.random.uniform(-0.05, 0.05), 
                                             np.random.uniform(-0.05, 0.05), 0])
        self.robot_id = p.loadURDF("quadruped/vision60.urdf", [0, 0, 0.55], rand_orn)
        
        self.step_count = 0
        for i, j_idx in enumerate(self.joint_indices):
            init_pos = 1.2 if i in [2, 5, 8, 11] else 0.0
            p.resetJointState(self.robot_id, j_idx, init_pos)
            
        return self._get_obs(), {}

    def _get_obs(self):
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        vel, _ = p.getBaseVelocity(self.robot_id)
        euler = p.getEulerFromQuaternion(orn)
        joint_angles = [p.getJointState(self.robot_id, i)[0] for i in self.joint_indices]
        phase = np.sin(2 * np.pi * 1.5 * (self.step_count * 0.01)) 
        return np.array([pos[2], vel[2]] + list(euler) + joint_angles + [phase], dtype=np.float32)

    def step(self, action):
        self.step_count += 1
        t = self.step_count * 0.01
        phase_a = np.sin(2 * np.pi * 1.5 * t)
        phase_b = -phase_a

        # é–¢ç¯€åˆ¶å¾¡
        for i, j_idx in enumerate(self.joint_indices):
            target_pos = 1.2 
            if i in [2, 8]: target_pos += phase_a * 0.4   # FR, RL
            elif i in [5, 11]: target_pos += phase_b * 0.4 # FL, RR
            
            target_pos += action[i] * 0.2
            target_pos = np.clip(target_pos, 0.1, 3.0)
            p.setJointMotorControl2(self.robot_id, j_idx, p.POSITION_CONTROL, targetPosition=target_pos, force=180.0)

        p.stepSimulation()
        obs = self._get_obs()
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        euler = p.getEulerFromQuaternion(orn)
        
        # çµ‚äº†åˆ¤å®šï¼ˆå³æ ¼åŒ–ï¼‰
        is_falling = pos[2] < 0.35 or abs(euler[0]) > 0.4 or abs(euler[1]) > 0.5
        terminated = False
        reward = 1.0 
        
        if self.step_count > 20 and is_falling:
            terminated = True
            reward = -100.0
        
        # å ±é…¬è¨ˆç®—ã¨ãƒ­ã‚°ç”¨å¤‰æ•°ã®æº–å‚™
        knee_fl = p.getJointState(self.robot_id, self.joint_indices[5])[0]
        knee_fr = p.getJointState(self.robot_id, self.joint_indices[2])[0]
        knee_diff = abs(knee_fl - knee_fr)
        
        if not terminated:
            reward -= abs(pos[2] - 0.45) * 15.0
            reward -= abs(euler[0]) * 30.0 # æ¨ªæºã‚ŒãƒšãƒŠãƒ«ãƒ†ã‚£
            
            # å·¦å³å¯¾ç§°ç¦æ­¢å ±é…¬ï¼šå·¦å³ã®å·®ãŒå¤§ãã„ã»ã©åŠ ç‚¹
            reward += (knee_diff - 0.2) * 10.0 
            
            vel, _ = p.getBaseVelocity(self.robot_id)
            reward += vel[0] * 50.0

        # TensorBoardã§è¦‹ãŸã„æ•°å€¤ã‚’infoã«å…¥ã‚Œã‚‹
        info = {
            "custom/knee_diff": knee_diff,
            "custom/height": pos[2],
            "custom/roll": abs(euler[0])
        }

        return obs, reward, terminated, False, info

# --- 3. å­¦ç¿’å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ ---
if __name__ == "__main__":
    # GPUãŒãªã„å ´åˆã¯å˜ä¸€ã€ã‚ã‚‹å ´åˆã¯è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¨å¥¨
    env = Vision60TrotEnv() 
    
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1, 
        device="cpu", # GPUãŒãªã„å ´åˆã¯æ˜ç¤ºçš„ã«CPU
        tensorboard_log="./ppo_vision_logs/"
    )
    
    print("ğŸ¾ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚TensorBoardã‚’åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§èµ·å‹•ã—ã¦å¾…æ©Ÿã—ã¦ãã ã•ã„ã€‚")
    callback = TensorboardCallback()
    model.learn(total_timesteps=500000, callback=callback)
    model.save("ppo_vision60_trot_final")